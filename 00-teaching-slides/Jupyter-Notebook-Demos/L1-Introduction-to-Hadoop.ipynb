{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]\n",
      "  CLASSNAME            run the class named CLASSNAME\n",
      " or\n",
      "  where COMMAND is one of:\n",
      "  fs                   run a generic filesystem user client\n",
      "  version              print the version\n",
      "  jar <jar>            run a jar file\n",
      "                       note: please use \"yarn jar\" to launch\n",
      "                             YARN applications, not this command.\n",
      "  checknative [-a|-h]  check native hadoop and compression libraries availability\n",
      "  distcp <srcurl> <desturl> copy file or directories recursively\n",
      "  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive\n",
      "  classpath            prints the class path needed to get the\n",
      "  credential           interact with credential providers\n",
      "                       Hadoop jar and the required libraries\n",
      "  daemonlog            get/set the log level for each daemon\n",
      "  trace                view and modify Hadoop tracing settings\n",
      "\n",
      "Most commands print help when invoked w/o parameters.\n"
     ]
    }
   ],
   "source": [
    "hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-current\n"
     ]
    }
   ],
   "source": [
    "echo $HADOOP_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-current/lib/*:/usr/lib/tez-current/*:/usr/lib/tez-current/lib/*:/etc/ecm/tez-conf:/usr/lib/hadoop-current/lib/*:/usr/lib/tez-current/*:/usr/lib/tez-current/lib/*:/etc/ecm/tez-conf:/opt/apps/extra-jars/*:/usr/lib/spark-current/yarn/spark-2.3.1-yarn-shuffle.jar:/opt/apps/extra-jars/*:/usr/lib/spark-current/yarn/spark-2.3.1-yarn-shuffle.jar\n"
     ]
    }
   ],
   "source": [
    "echo $HADOOP_CLASSPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/etc/ecm/hadoop-conf\n"
     ]
    }
   ],
   "source": [
    "echo $HADOOP_CONF_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <local|namenode:port>      specify a namenode\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "bin/hadoop command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "255",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "hadoop fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "drwxr-x--x   - hadoop    hadoop          0 2018-11-19 17:36 /apps\n",
      "drwxrwxrwx   - flowagent hadoop          0 2018-11-19 17:34 /emr-flow\n",
      "drwxr-x--x   - hadoop    hadoop          0 2018-11-28 22:31 /spark-history\n",
      "drwxrwxrwx   - root      hadoop          0 2018-11-28 22:30 /tmp\n",
      "drwxr-x--t   - hadoop    hadoop          0 2018-11-23 08:27 /user\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "drwx------   - hadoop   hadoop          0 2018-11-19 17:37 /user/hadoop\n",
      "drwxr-x--x   - hansheng hadoop          0 2018-11-23 08:28 /user/hansheng\n",
      "drwxr-x--x   - hadoop   hadoop          0 2018-11-19 17:35 /user/hive\n",
      "drwxr-x--x   - lifeng   hadoop          0 2018-11-22 22:22 /user/lifeng\n",
      "drwxr-x--x   - student  hadoop          0 2018-11-20 21:49 /user/student\n",
      "drwxr-x--x   - teacher  hadoop          0 2018-11-20 21:49 /user/teacher\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -put /opt/apps/ecm/service/hive/2.3.3-1.0.2/package/apache-hive-2.3.3-1.0.2-bin/binary-package-licenses/asm-LICENSE ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r-----   2 lifeng hadoop       4181 2018-11-22 22:22 /user/lifeng/Untitled1.ipynb\n",
      "-rw-r-----   2 lifeng hadoop       1511 2018-11-28 22:33 /user/lifeng/license.txt\n",
      "drwxr-x--x   - lifeng hadoop          0 2018-11-20 22:04 /user/lifeng/test\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /user/lifeng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -mv asm-LICENSE license.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/cat\n"
     ]
    }
   ],
   "source": [
    "which cat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/wc\n"
     ]
    }
   ],
   "source": [
    "which wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/11/28 23:03:15 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 30 minutes.\n",
      "Moved: 'hdfs://emr-header-1.cluster-41697:9000/user/lifeng/output' to trash at: hdfs://emr-header-1.cluster-41697:9000/user/lifeng/.Trash/Current\n",
      "packageJobJar: [/tmp/hadoop-unjar2983953304765363597/] [] /tmp/streamjob1437159434611072692.jar tmpDir=null\n",
      "18/11/28 23:03:17 INFO impl.TimelineClientImpl: Timeline service address: http://emr-header-1.cluster-41697:8188/ws/v1/timeline/\n",
      "18/11/28 23:03:17 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-41697/192.168.0.219:8032\n",
      "18/11/28 23:03:18 INFO impl.TimelineClientImpl: Timeline service address: http://emr-header-1.cluster-41697:8188/ws/v1/timeline/\n",
      "18/11/28 23:03:18 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-41697/192.168.0.219:8032\n",
      "18/11/28 23:03:18 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries\n",
      "18/11/28 23:03:18 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 97184efe294f64a51a4c5c172cbc22146103da53]\n",
      "18/11/28 23:03:18 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/11/28 23:03:18 INFO mapreduce.JobSubmitter: number of splits:16\n",
      "18/11/28 23:03:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1542711134746_0007\n",
      "18/11/28 23:03:18 INFO impl.YarnClientImpl: Submitted application application_1542711134746_0007\n",
      "18/11/28 23:03:18 INFO mapreduce.Job: The url to track the job: http://emr-header-1.cluster-41697:20888/proxy/application_1542711134746_0007/\n",
      "18/11/28 23:03:18 INFO mapreduce.Job: Running job: job_1542711134746_0007\n",
      "18/11/28 23:03:23 INFO mapreduce.Job: Job job_1542711134746_0007 running in uber mode : false\n",
      "18/11/28 23:03:23 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/11/28 23:03:30 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/11/28 23:03:34 INFO mapreduce.Job:  map 100% reduce 14%\n",
      "18/11/28 23:03:35 INFO mapreduce.Job:  map 100% reduce 57%\n",
      "18/11/28 23:03:36 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "18/11/28 23:03:37 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/11/28 23:03:37 INFO mapreduce.Job: Job job_1542711134746_0007 completed successfully\n",
      "18/11/28 23:03:38 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1367\n",
      "\t\tFILE: Number of bytes written=3033145\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=14720\n",
      "\t\tHDFS: Number of bytes written=175\n",
      "\t\tHDFS: Number of read operations=69\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=14\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tData-local map tasks=16\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4241097\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1656720\n",
      "\t\tTotal time spent by all map tasks (ms)=71883\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14160\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=71883\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=14160\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=134564976\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=53015040\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=29\n",
      "\t\tMap output records=29\n",
      "\t\tMap output bytes=1540\n",
      "\t\tMap output materialized bytes=2961\n",
      "\t\tInput split bytes=1824\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=25\n",
      "\t\tReduce shuffle bytes=2961\n",
      "\t\tReduce input records=29\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=58\n",
      "\t\tShuffled Maps =112\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=112\n",
      "\t\tGC time elapsed (ms)=3233\n",
      "\t\tCPU time spent (ms)=29340\n",
      "\t\tPhysical memory (bytes) snapshot=10728050688\n",
      "\t\tVirtual memory (bytes) snapshot=95547158528\n",
      "\t\tTotal committed heap usage (bytes)=17288396800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=12896\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=175\n",
      "18/11/28 23:03:38 INFO streaming.StreamJob: Output directory: /user/lifeng/output\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -rm -r /user/lifeng/output\n",
    "\n",
    "hadoop jar /opt/apps/ecm/service/hadoop/2.7.2-1.3.1/package/hadoop-2.7.2-1.3.1/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar -input /user/lifeng/license.txt  -output /user/lifeng/output -mapper \"/usr/bin/cat\" -reducer \"/usr/bin/wc\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\n",
      "-rw-r-----   2 lifeng hadoop          0 2018-11-28 23:03 /user/lifeng/output/_SUCCESS\n",
      "-rw-r-----   2 lifeng hadoop         25 2018-11-28 23:03 /user/lifeng/output/part-00000\n",
      "-rw-r-----   2 lifeng hadoop         25 2018-11-28 23:03 /user/lifeng/output/part-00001\n",
      "-rw-r-----   2 lifeng hadoop         25 2018-11-28 23:03 /user/lifeng/output/part-00002\n",
      "-rw-r-----   2 lifeng hadoop         25 2018-11-28 23:03 /user/lifeng/output/part-00003\n",
      "-rw-r-----   2 lifeng hadoop         25 2018-11-28 23:03 /user/lifeng/output/part-00004\n",
      "-rw-r-----   2 lifeng hadoop         25 2018-11-28 23:03 /user/lifeng/output/part-00005\n",
      "-rw-r-----   2 lifeng hadoop         25 2018-11-28 23:03 /user/lifeng/output/part-00006\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /user/lifeng/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6      63     375\t\n",
      "      7      24     159\t\n",
      "      5      39     290\t\n",
      "      4      39     279\t\n",
      "      3      21     150\t\n",
      "      2      20     147\t\n",
      "      2      19     140\t\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -cat /user/lifeng/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/11/28 23:02:19 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 30 minutes.\n",
      "Moved: 'hdfs://emr-header-1.cluster-41697:9000/user/lifeng/output' to trash at: hdfs://emr-header-1.cluster-41697:9000/user/lifeng/.Trash/Current\n",
      "packageJobJar: [/tmp/hadoop-unjar5042604648487057074/] [] /tmp/streamjob438820979467674195.jar tmpDir=null\n",
      "18/11/28 23:02:21 INFO impl.TimelineClientImpl: Timeline service address: http://emr-header-1.cluster-41697:8188/ws/v1/timeline/\n",
      "18/11/28 23:02:21 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-41697/192.168.0.219:8032\n",
      "18/11/28 23:02:21 INFO impl.TimelineClientImpl: Timeline service address: http://emr-header-1.cluster-41697:8188/ws/v1/timeline/\n",
      "18/11/28 23:02:21 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-41697/192.168.0.219:8032\n",
      "18/11/28 23:02:21 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries\n",
      "18/11/28 23:02:21 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 97184efe294f64a51a4c5c172cbc22146103da53]\n",
      "18/11/28 23:02:21 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/11/28 23:02:21 INFO mapreduce.JobSubmitter: number of splits:16\n",
      "18/11/28 23:02:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1542711134746_0006\n",
      "18/11/28 23:02:22 INFO impl.YarnClientImpl: Submitted application application_1542711134746_0006\n",
      "18/11/28 23:02:22 INFO mapreduce.Job: The url to track the job: http://emr-header-1.cluster-41697:20888/proxy/application_1542711134746_0006/\n",
      "18/11/28 23:02:22 INFO mapreduce.Job: Running job: job_1542711134746_0006\n",
      "18/11/28 23:02:27 INFO mapreduce.Job: Job job_1542711134746_0006 running in uber mode : false\n",
      "18/11/28 23:02:27 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/11/28 23:02:33 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/11/28 23:02:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/11/28 23:02:38 INFO mapreduce.Job: Job job_1542711134746_0006 completed successfully\n",
      "18/11/28 23:02:38 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=895\n",
      "\t\tFILE: Number of bytes written=2239632\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=14720\n",
      "\t\tHDFS: Number of bytes written=25\n",
      "\t\tHDFS: Number of read operations=51\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=16\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4232542\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=231543\n",
      "\t\tTotal time spent by all map tasks (ms)=71738\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1979\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=71738\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1979\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=134293536\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=7409376\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=29\n",
      "\t\tMap output records=29\n",
      "\t\tMap output bytes=1540\n",
      "\t\tMap output materialized bytes=1535\n",
      "\t\tInput split bytes=1824\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=25\n",
      "\t\tReduce shuffle bytes=1535\n",
      "\t\tReduce input records=29\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=58\n",
      "\t\tShuffled Maps =16\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=16\n",
      "\t\tGC time elapsed (ms)=2932\n",
      "\t\tCPU time spent (ms)=23700\n",
      "\t\tPhysical memory (bytes) snapshot=8758489088\n",
      "\t\tVirtual memory (bytes) snapshot=63770599424\n",
      "\t\tTotal committed heap usage (bytes)=12781092864\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=12896\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=25\n",
      "18/11/28 23:02:38 INFO streaming.StreamJob: Output directory: /user/lifeng/output\n",
      "     29     225    1540\t\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -rm -r /user/lifeng/output\n",
    "hadoop jar /opt/apps/ecm/service/hadoop/2.7.2-1.3.1/package/hadoop-2.7.2-1.3.1/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar -input /user/lifeng/license.txt  -output /user/lifeng/output -mapper \"/usr/bin/cat\" -reducer \"/usr/bin/wc\" -numReduceTasks 1\n",
    "hadoop fs -cat /user/lifeng/output/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
