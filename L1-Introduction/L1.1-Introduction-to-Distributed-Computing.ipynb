{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Distributed Computing\n",
    "\n",
    "## Feng Li\n",
    "\n",
    "### Central University of Finance and Economics\n",
    "\n",
    "### [feng.li@cufe.edu.cn](feng.li@cufe.edu.cn)\n",
    "### Course home page: [https://feng.li/distcomp](https://feng.li/distcomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Distributed Systems\n",
    "\n",
    "- Moore’s law suited us well for the past decades.\n",
    "\n",
    "- But building bigger and bigger single servers (like IBM supercomputer) is no longer necessarily the best solution to large-scale problems in industry.\n",
    "\n",
    "- An alternative that has gained popularity is to tie together many low-end/commodity machines together as a single functional **distributed system**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributed Computing is Eveywhere \n",
    "\n",
    "\n",
    "![Search](./figures/eg1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Stocks](./figures/eg2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image](./figures/eg3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The performance of distributed systems\n",
    "\n",
    "\n",
    "- A high-end machine with four I/O channels each having a throughput of 100 MB/sec will require three hours to read a 4 TB data set! \n",
    "\n",
    "\n",
    "- With a distributed system, this same data set will be divided into smaller (typically 64 MB) blocks that are spread among many machines in the cluster via the **Distributed File System**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The _move-code-to-data_ philosophy\n",
    "\n",
    "- The traditional supercomputer requires repeat transmissions of data between clients and servers. This works fine for computationally intensive work, but for data-intensive processing, the size of data becomes too large to be moved around easily. \n",
    "\n",
    "\n",
    "- A distributed systems focuses on **moving code to data**. \n",
    "\n",
    "- The clients send only the programs to be executed, and these programs are usually small.\n",
    "\n",
    "- More importantly, data are broken up and distributed across the cluster, and as much as possible, computation on a piece of data takes place on the same machine where that piece of data resides.\n",
    "\n",
    "- The whole process is known as **MapReduce**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The ecosystem for distributed computing\n",
    "\n",
    "![image](./figures/hadoop_ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Hadoop?\n",
    "\n",
    "- Hadoop is a platform that provides both distributed storage and computational capabilities.\n",
    "\n",
    "- Hadoop is a distributed master-slave architecture consists of the **Hadoop Distributed File System (HDFS)** for storage and **MapReduce** for computational capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Brief History of Hadoop\n",
    "\n",
    "- Hadoop was created by Doug Cutting.\n",
    "\n",
    "- At the time Google had published papers that described its novel distributed filesystem, the Google File System ( GFS ), and MapReduce, a computational framework for parallel processing.\n",
    "\n",
    "- The successful implementation of these papers’ concepts resulted in the Hadoop project.\n",
    "\n",
    "- Who use Hadoop?\n",
    "\n",
    "    - Facebook uses Hadoop, Hive, and HB ase for data warehousing and real-time application serving.\n",
    "    - Twitter uses Hadoop, Pig, and HB ase for data analysis, visualization, social graph analysis, and machine learning.\n",
    "    - Yahoo! uses Hadoop for data analytics, machine learning, search ranking, email antispam, ad optimization...\n",
    "    - eBay, Samsung, Rackspace, J.P. Morgan, Groupon, LinkedIn, AOL , Last.fm..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**But we (statisticians, financial analysts) are not yet there!**\n",
    "\n",
    "--- and we should!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![hadoop-architecture](./figures/hadoop-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Core Hadoop components: HDFS \n",
    "\n",
    "- HDFS is the storage component of Hadoop\n",
    "\n",
    "- It’s a distributed file system.\n",
    "- Logical representation of the components in HDFS : the **NameNode** and the **DataNode**.\n",
    "\n",
    "- HDFS replicates files for a configured number of times, is tolerant of both software and hardware failure, and automatically re-replicates data blocks on nodes that have failed.\n",
    "\n",
    "- HDFS isn’t designed to work well with random reads over small files due to its optimization for sustained throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![mapreduce](./figures/hdfs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Core Hadoop components: MapReduce\n",
    "\n",
    "\n",
    "- MapReduce is a batch-based, distributed computing framework.\n",
    "- It allows you to parallelize work over a large amount of raw data.\n",
    "- This type of work, which could take days or longer using conventional serial programming techniques, can be reduced down to minutes using MapReduce on a Hadoop cluster.\n",
    "- MapReduce allows the programmer to focus on addressing business needs, rather than getting tangled up in distributed system complications.\n",
    "- MapReduce doesn’t lend itself to use cases that need real-time data access.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![mapreduce](./figures/mapreduce-architecture.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
