{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce with Hadoop Streaming\n",
    "\n",
    "## Feng Li\n",
    "\n",
    "### Central University of Finance and Economics\n",
    "\n",
    "### [feng.li@cufe.edu.cn](feng.li@cufe.edu.cn)\n",
    "### Course home page: [https://feng.li/distcomp](https://feng.li/distcomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hadoop Streaming, `stdin` and `stdout`\n",
    "\n",
    "- Hadoop provides an API to MapReduce that allows you to write your map and reduce functions in languages other than Java\n",
    "\n",
    "- Hadoop Streaming uses Unix standard streams as the interface between Hadoop and your program, so you can use any combination of languages that can read standard input and write to standard output to write your MapReduce program.\n",
    "\n",
    "    - You could use different language in mapper and reduce functions.\n",
    "\n",
    "    - It suits for text processing (e.g. read every line from a big CSV file).\n",
    "\n",
    "    - It can also handle binary streams (e.g. read image as input)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Best Practice with Hadoop Streaming\n",
    "\n",
    "- Write you Hadoop commnad in a Bash file instead run it directly on Linux Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "PWD=$(cd $(dirname $0); pwd)\n",
      "cd $PWD 1> /dev/null 2>&1\n",
      "\n",
      "TASKNAME=task1_fli\n",
      "# python location on hadoop\n",
      "PY27='/fli/tools/python2.7.tar.gz'\n",
      "# hadoop client\n",
      "HADOOP_HOME=/home/users/fli/hadoop/bin/hadoop\n",
      "HADOOP_INPUT_DIR1=/fli/data1/part-*\n",
      "HADOOP_INPUT_DIR2=/fli/data2/part-*\n",
      "HADOOP_OUTPUT_DIR=/fli/results/task1\n",
      "\n",
      "echo $HADOOP_HOME\n",
      "echo $HADOOP_INPUT_DIR\n",
      "echo $HADOOP_OUTPUT_DIR\n",
      "\n",
      "$HADOOP_HOME fs -rmr $HADOOP_OUTPUT_DIR\n",
      "\n",
      "$HADOOP_HOME streaming \\\n",
      "    -jobconf mapred.job.name=$TASKNAME \\\n",
      "    -jobconf mapred.job.priority=NORMAL \\\n",
      "    -jobconf mapred.map.tasks=500 \\\n",
      "    -jobconf mapred.reduce.tasks=500 \\\n",
      "    -jobconf mapred.job.map.capacity=500 \\\n",
      "    -jobconf mapred.job.reduce.capacity=500 \\\n",
      "    -jobconf stream.num.map.output.key.fields=2 \\\n",
      "    -jobconf mapred.text.key.partitioner.options=-k1,1 \\\n",
      "    -jobconf stream.memory.limit=1000 \\\n",
      "    -file $PWD/mapper.sh \\\n",
      "    -output ${HADOOP_OUTPUT_DIR} \\\n",
      "    -input ${HADOOP_INPUT_DIR1} \\\n",
      "    -input ${HADOOP_INPUT_DIR2} \\\n",
      "    -mapper \"sh mapper.sh\" \\\n",
      "    -reducer \"cat\" \\\n",
      "    -cacheArchive ${PY27}#py27 \\\n",
      "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
      "\n",
      "\n",
      "if [ $? -ne 0 ]; then\n",
      "    echo 'error'\n",
      "    exit 1\n",
      "fi\n",
      "$HADOOP_HOME fs -touchz ${HADOOP_OUTPUT_DIR}/done\n",
      "\n",
      "exit 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat examples/print-colums/main.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce with Examples\n",
    "\n",
    "\n",
    "- Mapper: Bash, Reducer: Python\n",
    "\n",
    "- Write MapReduce with Python\n",
    "\n",
    "- Write MapReduce with R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other Hadoop APIs\n",
    "\n",
    "- Hadoop with Java MapReduce\n",
    "    - Hadoop is written in Java. There are rich classes of Java MapReduce modules ready to use.\n",
    "    - You need javac (in JDK) and hadoop-mapreduce-client-core-xxx.jar to comile your jar files.\n",
    "    \n",
    "    ```sh\n",
    "    javac -classpath $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.2.jar -d FirstJar\\\n",
    "\n",
    "    jar -cvf FirstJar.jar -C FirstJar/\n",
    "    ```\n",
    "\n",
    "- The Java version Hadoop syntax is\n",
    "\n",
    "```sh\n",
    "hadoop jar FirstJar.jar [mainClass] input output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Hadoop Pipe\n",
    "    - Hadoop Pipes is the name of the C++ interface to Hadoop MapReduce.\n",
    "    - Pipes uses sockets as the channel over which the tasktracker communicates with the process running the C++ map or reduce function.\n",
    "    - The application links against the Hadoop C++ library, which is a thin wrapper for communicating with the tasktracker child process.\n",
    "\n",
    "- You have to compile and link your C++ program before send it to Hadoop.\n",
    "\n",
    "- The Hadoop Pipe syntax\n",
    "```sh\n",
    "hadoop pipes \\\n",
    "    -D hadoop.pipes.java.recordreader=true \\\n",
    "    -D hadoop.pipes.java.recordwriter=true \\\n",
    "    -input sample.txt \\\n",
    "    -output output \\\n",
    "    -program myCPProgram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab 2 \n",
    "\n",
    "- Use `airline_small.csv` as input. The data description is available at http://stat-computing.org/dataexpo/2009/the-data.html\n",
    "\n",
    "- Extract useful information from the data\n",
    "\n",
    "    - List all airport codes, with frequency\n",
    "    - Make a new binary variable (Y) to indicate if a trip is delayed or not.\n",
    "    \n",
    "- Make dummy transformation for variables such as `DayofWeek`, `Month`...\n",
    "\n",
    "- Finally, save your output in a file.\n",
    "\n",
    "    - Each row contains the binary variable (Y), `CarrierDelay`, and your constructed dummy variables as predictors.\n",
    "    - If possible, save the output in a [`libsvm` sparse format](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.dump_svmlight_file.html#sklearn.datasets.dump_svmlight_file) to save space.\n",
    "    \n",
    "\n",
    "- **Hint**\n",
    "\n",
    "    - You could use any language but Python3.7 is preferable.\n",
    "    - Try your code with pipe first and then Hadoop\n",
    "    - Record the computing time."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
