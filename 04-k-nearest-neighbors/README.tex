\section{KNN算法}\label{ux5b9eux4f8bux5206ux6790mapreduce-ux6846ux67b6ux4e0bux7684-knn-ux7b97ux6cd5ux5b9eux73b0}

\subsection{准备知识}\label{ux51c6ux5907ux77e5ux8bc6}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  KNN分类算法
\item
  R
\item
  Hadoop Streaming
\end{itemize}

\subsection{K最近邻算法（KNN）}\label{kux6700ux8fd1ux90bbux7b97ux6cd5knn}

k-最近邻分类法是一个理论上比较成熟的机器学习算法，该方法的思路是：如果一个样本在特征空间
中 的 k 个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类
别。 KNN 算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或
者几个样本的类别来决定待分样本所属的类别。

\subsection{数据介绍}\label{ux6570ux636eux4ecbux7ecd}
报告中的数据来源于经典的安德森鸢尾花卉数据集，其数据集包含了150个样本，5个变量。因变量为鸢
尾花的3个属种，四个自变量分别为花萼和花瓣的长度和宽度。使用鸢尾花的这四个定量特征来对其所属
的3个属种进行判别分析，这是统计学中的经典案例。

\subsection{建立KNN分类模型}\label{ux5efaux7acbknnux5206ux7c7bux6a21ux578b}

\subsubsection{Mapper函数}\label{mapperux51fdux6570}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  需对所有样本点（训练集+测试集）进行归一化处理。
  然后，对未知分类的数据集中的每个样本点依次 执行以下操作。
\item
  计算训练集中的点与当前点（测试集）的距离。
\item
  按照距离递增排序。
\item
  选取与当前距离最小的 k(取 5)个点。
\item
  Map 函数的输出为：测试集实际的分类及与当前距离最近的 k 个 点所属类别。

\begin{lstlisting}
	#! /usr/bin/env Rscript
	sink("/dev/null")
	# 读取iris数据集
	input <- file("stdin","r")
	rawdata <- read.table(input)
	k = 5
	# 归一化
	data <- cbind(as.data.frame(scale(rawdata[,1:4])),
	            Species=raw data[,5])
	# 随机选择其中的120条记录作为训练集,30条作为测试集
	set.seed(1234)
	sampleid <- sample(1:150,120)
	train <- data[sampleid,]
	test <- data[-sampleid,]
	#对测试集的每一个样本执行如下步骤
	for(i in 1:nrow(test)){
	    #记录与每个训练集的距离及该样本的分类
	    dis2train <- matrix(rep(0,nrow(train)*2),ncol=2)
	    # 计算距离并保存训练集的分类信息
	    for(j in 1:nrow(train)){
	        dis2train[j,1] <- dist(rbind(test[i,1:4],train[j,1:4]),method="euclidean")
	        dis2train[j,2] <- train[j,5]
	    }
	    #按距离从小到大排序
	    sorteddis2train<-dis2train[order(dis2train[,1]),]
	    sink()
	    #将测试集实际的分类及与当前距离最近的k个点所属类别
	    cat(test[i,5],sorteddis2train[1:k,2],"\n",sep="\t")
	    sink("/dev/null")
	}
	close(input)
\end{lstlisting}
\end{enumerate}

\subsubsection{Reducer 函数}\label{reducer-ux51fdux6570}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  前 k 个点出现频率最高的类别作为当前点的预测类别。
\item
  Reduce 函数的输出为:测试集的实际分类及预测分类。

\begin{lstlisting}
	#! /usr/bin/env Rscript
	sink("/dev/null")
	#读取map的输出
	input <- file("stdin","r")
	species <- read.table(input)
	for(i in 1:nrow(species)){
	    #计算频率
	    freq <- as.data.frame(table(species[i,2:5]))
	    #将频率从大到小排列
	    sortedfreq <- freq[order(-freq$Freq),]
	    sink()
	    #输出测试集的实际分类及预测分类
	    cat(as.character(species[i,1]),
	        as.character(sortedfreq[1,1] ),'\n',sep='\t')
	    sink("/dev/null")
	}
	close(input)
\end{lstlisting}
\end{enumerate}
模拟 Hadoop 模式调试:

\begin{lstlisting}
	$ cat iris.csv | python mapper.py | python reducer.py
\end{lstlisting}
如果在Hadoop集群运行:

\begin{lstlisting}
	$ hadoop jar /home/dmc/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \
	-file /home/dmc/*.py \
	-input iris.csv \
	-output output \
	-mapper "mapper.py" \
	-reducer "reducer.py"
\end{lstlisting}

\subsection{实验结果}\label{ux5b9eux9a8cux7ed3ux679c}



使用测试集测试 k 近邻模型的准确度,混淆矩阵如下：

\begin{lstlisting}
	             setosa versicolor virginica
	setosa         11        0         0
	versicolor      0        12        1
	virginica       0        1         5
\end{lstlisting}

{\color{red}从测试结果的混淆矩阵可以看出，测试结果是比较理想的，在 MapReduce 框架下也可以实现KNN算法。}
