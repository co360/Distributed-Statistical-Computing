{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Hadoop\n",
    "## Feng Li\n",
    "\n",
    "### Central University of Finance and Economics\n",
    "\n",
    "### [feng.li@cufe.edu.cn](feng.li@cufe.edu.cn)\n",
    "### Course home page: [https://feng.li/distcomp](https://feng.li/distcomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The building blocks of Hadoop\n",
    "\n",
    "- NameNode\n",
    "- DataNode\n",
    "- Secondary NameNode\n",
    "- JobTracker\n",
    "- TaskTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NameNode\n",
    "\n",
    "- The NameNode is the master of HDFS that directs the worker DataNode daemons to perform the low-level I/O tasks.\n",
    "\n",
    "- The NameNode keeps track of how your fi les are broken down into fi le blocks, which nodes store those blocks, and the overall health of the distributed file system.\n",
    "\n",
    "- The NameNode is a single point of failure of your Hadoop cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Secondary NameNode\n",
    "\n",
    "- An assistant daemon for monitoring the state of the cluster HDFS.\n",
    "\n",
    "- Each cluster has one Secondary NameNode.\n",
    "\n",
    "- The secondary NameNode snapshots help minimize the downtime and loss of data due to the failure of NameNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DataNode\n",
    "\n",
    "- Each worker machine in your cluster will host a DataNode daemon to perform the grunt work of the distributed file system -- reading and writing HDFS blocks to actual files on the local file system.\n",
    "\n",
    "- DataNodes are constantly reporting to the NameNode.\n",
    "\n",
    "- Each of the DataNodes informs the NameNode of the blocks it’s currently storing. After this mapping is complete, the DataNodes continually poll the NameNode to provide information regarding local changes as well as receive instructions to create, move, or delete blocks from the local disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## JobTracker\n",
    "\n",
    "- There is only one JobTracker daemon per Hadoop cluster. It’s typically run on a server as a master node of the cluster.\n",
    "\n",
    "- The JobTracker determines the execution plan by determining which fi les to process, assigns nodes to different tasks, and monitors all tasks as they’re running. Should a task fail, the JobTracker will automatically relaunch the task, possibly on a different node, up to a predefi ned limit of retries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TaskTracker\n",
    "\n",
    "\n",
    "- Each TaskTracker is responsible for executing the individual tasks that the JobTracker assigns.\n",
    "\n",
    "- If the JobTracker fails to receive a heartbeat from a TaskTracker within a specified amount of time, it will assume the TaskTracker has crashed and will resubmit the corresponding tasks to other nodes in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Hadoop \n",
    "\n",
    "- Basic requirements\n",
    "\n",
    "    - Linux machines \n",
    "    - SSH \n",
    "    - Setup environment variables: `JAVA_HOME` , `HADOOP_HOME`\n",
    "\n",
    "- Cluster setup: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html\n",
    "\n",
    "- Core Hadoop configuration files\n",
    "\n",
    "    - `core-site.xml`\n",
    "    - `mapred-site.xml`\n",
    "    - `hdfs-site.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Work with Hadoop File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]\n",
      "  CLASSNAME            run the class named CLASSNAME\n",
      " or\n",
      "  where COMMAND is one of:\n",
      "  fs                   run a generic filesystem user client\n",
      "  version              print the version\n",
      "  jar <jar>            run a jar file\n",
      "                       note: please use \"yarn jar\" to launch\n",
      "                             YARN applications, not this command.\n",
      "  checknative [-a|-h]  check native hadoop and compression libraries availability\n",
      "  distcp <srcurl> <desturl> copy file or directories recursively\n",
      "  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive\n",
      "  classpath            prints the class path needed to get the\n",
      "  credential           interact with credential providers\n",
      "                       Hadoop jar and the required libraries\n",
      "  daemonlog            get/set the log level for each daemon\n",
      "  trace                view and modify Hadoop tracing settings\n",
      "\n",
      "Most commands print help when invoked w/o parameters.\n"
     ]
    }
   ],
   "source": [
    "hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 2.7.2\n",
      "Subversion git@gitlab.alibaba-inc.com:soe/emr-hadoop.git -r 01979868624477e85d8958501eb27a460ce81e4c\n",
      "Compiled by root on 2018-08-31T09:14Z\n",
      "Compiled with protoc 2.5.0\n",
      "From source with checksum 4447ed9f24dcd981df7daaadd5bafc0\n",
      "This command was run using /opt/apps/ecm/service/hadoop/2.7.2-1.3.1/package/hadoop-2.7.2-1.3.1/share/hadoop/common/hadoop-common-2.7.2.jar\n"
     ]
    }
   ],
   "source": [
    "hadoop version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-1.8.0\n"
     ]
    }
   ],
   "source": [
    "echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-current\n"
     ]
    }
   ],
   "source": [
    "echo $HADOOP_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-current/lib/*:/usr/lib/tez-current/*:/usr/lib/tez-current/lib/*:/etc/ecm/tez-conf:/usr/lib/hadoop-current/lib/*:/usr/lib/tez-current/*:/usr/lib/tez-current/lib/*:/etc/ecm/tez-conf:/opt/apps/extra-jars/*:/usr/lib/spark-current/yarn/spark-2.4.4-yarn-shuffle.jar:/opt/apps/extra-jars/*:/usr/lib/spark-current/yarn/spark-2.4.4-yarn-shuffle.jar\n"
     ]
    }
   ],
   "source": [
    "echo $HADOOP_CLASSPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/etc/ecm/hadoop-conf\n"
     ]
    }
   ],
   "source": [
    "echo $HADOOP_CONF_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <local|namenode:port>      specify a namenode\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "bin/hadoop command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "255",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "hadoop fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "drwxr-x--x   - hadoop    hadoop          0 2020-01-06 13:27 /apps\n",
      "drwxr-x--x   - lifeng    hadoop          0 2020-02-20 10:54 /data\n",
      "drwxrwxrwx   - flowagent hadoop          0 2020-01-06 13:27 /emr-flow\n",
      "drwxr-x--x   - hadoop    hadoop          0 2020-02-10 22:20 /spark-history\n",
      "drwxrwxrwx   - root      hadoop          0 2020-02-26 21:36 /tmp\n",
      "drwxr-x--t   - hadoop    hadoop          0 2020-02-22 15:56 /user\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwx------   - hadoop  hadoop          0 2020-01-06 13:29 /user/hadoop\n",
      "drwxr-x--x   - hadoop  hadoop          0 2020-01-06 13:27 /user/hive\n",
      "drwxr-x--x   - lifeng  hadoop          0 2020-02-21 12:06 /user/lifeng\n",
      "drwx------   - student hadoop          0 2020-02-22 15:28 /user/student\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "hadoop fs -put /opt/apps/ecm/service/hive/2.3.3-1.0.2/package/apache-hive-2.3.3-1.0.2-bin/binary-package-licenses/asm-LICENSE ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-x--x   - lifeng hadoop          0 2020-02-10 22:20 /user/lifeng/.sparkStaging\n",
      "-rw-r-----   2 lifeng hadoop       1511 2020-02-26 21:36 /user/lifeng/asm-LICENSE\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /user/lifeng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "hadoop fs -mv asm-LICENSE license.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Your first Hadoop MapReduce program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/cat\n"
     ]
    }
   ],
   "source": [
    "which cat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/wc\n"
     ]
    }
   ],
   "source": [
    "which wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/lifeng/output': No such file or directory\n",
      "packageJobJar: [/tmp/hadoop-unjar7580110097437608263/] [] /tmp/streamjob1753312041067415791.jar tmpDir=null\n",
      "20/02/26 21:41:52 INFO impl.TimelineClientImpl: Timeline service address: http://emr-header-1.cluster-46968:8188/ws/v1/timeline/\n",
      "20/02/26 21:41:52 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-46968/192.168.0.222:8032\n",
      "20/02/26 21:41:52 INFO impl.TimelineClientImpl: Timeline service address: http://emr-header-1.cluster-46968:8188/ws/v1/timeline/\n",
      "20/02/26 21:41:52 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-46968/192.168.0.222:8032\n",
      "20/02/26 21:41:53 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries\n",
      "20/02/26 21:41:53 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 97184efe294f64a51a4c5c172cbc22146103da53]\n",
      "20/02/26 21:41:53 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/02/26 21:41:53 INFO mapreduce.JobSubmitter: number of splits:16\n",
      "20/02/26 21:41:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579055927679_0005\n",
      "20/02/26 21:41:53 INFO impl.YarnClientImpl: Submitted application application_1579055927679_0005\n",
      "20/02/26 21:41:53 INFO mapreduce.Job: The url to track the job: http://emr-header-1.cluster-46968:20888/proxy/application_1579055927679_0005/\n",
      "20/02/26 21:41:53 INFO mapreduce.Job: Running job: job_1579055927679_0005\n",
      "20/02/26 21:41:58 INFO mapreduce.Job: Job job_1579055927679_0005 running in uber mode : false\n",
      "20/02/26 21:41:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/02/26 21:42:06 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/02/26 21:42:11 INFO mapreduce.Job:  map 100% reduce 29%\n",
      "20/02/26 21:42:12 INFO mapreduce.Job:  map 100% reduce 57%\n",
      "20/02/26 21:42:13 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "20/02/26 21:42:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/02/26 21:42:14 INFO mapreduce.Job: Job job_1579055927679_0005 completed successfully\n",
      "20/02/26 21:42:14 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1367\n",
      "\t\tFILE: Number of bytes written=3035077\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=14720\n",
      "\t\tHDFS: Number of bytes written=175\n",
      "\t\tHDFS: Number of read operations=69\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=14\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tData-local map tasks=16\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4947858\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1907919\n",
      "\t\tTotal time spent by all map tasks (ms)=83862\n",
      "\t\tTotal time spent by all reduce tasks (ms)=16307\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=83862\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=16307\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=156989664\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=61053408\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=29\n",
      "\t\tMap output records=29\n",
      "\t\tMap output bytes=1540\n",
      "\t\tMap output materialized bytes=2961\n",
      "\t\tInput split bytes=1824\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=25\n",
      "\t\tReduce shuffle bytes=2961\n",
      "\t\tReduce input records=29\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=58\n",
      "\t\tShuffled Maps =112\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=112\n",
      "\t\tGC time elapsed (ms)=3117\n",
      "\t\tCPU time spent (ms)=33410\n",
      "\t\tPhysical memory (bytes) snapshot=10298241024\n",
      "\t\tVirtual memory (bytes) snapshot=92690731008\n",
      "\t\tTotal committed heap usage (bytes)=16765157376\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=12896\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=175\n",
      "20/02/26 21:42:14 INFO streaming.StreamJob: Output directory: /user/lifeng/output\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -rm -r /user/lifeng/output\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar -input /user/lifeng/asm-LICENSE  -output /user/lifeng/output -mapper \"/bin/cat\" -reducer \"/bin/wc\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\n",
      "-rw-r-----   2 lifeng hadoop          0 2020-02-26 21:42 /user/lifeng/output/_SUCCESS\n",
      "-rw-r-----   2 lifeng hadoop         25 2020-02-26 21:42 /user/lifeng/output/part-00000\n",
      "-rw-r-----   2 lifeng hadoop         25 2020-02-26 21:42 /user/lifeng/output/part-00001\n",
      "-rw-r-----   2 lifeng hadoop         25 2020-02-26 21:42 /user/lifeng/output/part-00002\n",
      "-rw-r-----   2 lifeng hadoop         25 2020-02-26 21:42 /user/lifeng/output/part-00003\n",
      "-rw-r-----   2 lifeng hadoop         25 2020-02-26 21:42 /user/lifeng/output/part-00004\n",
      "-rw-r-----   2 lifeng hadoop         25 2020-02-26 21:42 /user/lifeng/output/part-00005\n",
      "-rw-r-----   2 lifeng hadoop         25 2020-02-26 21:42 /user/lifeng/output/part-00006\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -ls /user/lifeng/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6      63     375\t\n",
      "      7      24     159\t\n",
      "      5      39     290\t\n",
      "      4      39     279\t\n",
      "      3      21     150\t\n",
      "      2      20     147\t\n",
      "      2      19     140\t\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -cat /user/lifeng/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/02/26 21:43:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 30 minutes.\n",
      "Moved: 'hdfs://emr-header-1.cluster-46968:9000/user/lifeng/output' to trash at: hdfs://emr-header-1.cluster-46968:9000/user/lifeng/.Trash/Current\n",
      "packageJobJar: [/tmp/hadoop-unjar6344502777978736495/] [] /tmp/streamjob5520248859546438609.jar tmpDir=null\n",
      "20/02/26 21:43:24 INFO impl.TimelineClientImpl: Timeline service address: http://emr-header-1.cluster-46968:8188/ws/v1/timeline/\n",
      "20/02/26 21:43:24 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-46968/192.168.0.222:8032\n",
      "20/02/26 21:43:24 INFO impl.TimelineClientImpl: Timeline service address: http://emr-header-1.cluster-46968:8188/ws/v1/timeline/\n",
      "20/02/26 21:43:24 INFO client.RMProxy: Connecting to ResourceManager at emr-header-1.cluster-46968/192.168.0.222:8032\n",
      "20/02/26 21:43:24 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries\n",
      "20/02/26 21:43:24 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 97184efe294f64a51a4c5c172cbc22146103da53]\n",
      "20/02/26 21:43:24 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/02/26 21:43:24 INFO mapreduce.JobSubmitter: number of splits:16\n",
      "20/02/26 21:43:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579055927679_0006\n",
      "20/02/26 21:43:25 INFO impl.YarnClientImpl: Submitted application application_1579055927679_0006\n",
      "20/02/26 21:43:25 INFO mapreduce.Job: The url to track the job: http://emr-header-1.cluster-46968:20888/proxy/application_1579055927679_0006/\n",
      "20/02/26 21:43:25 INFO mapreduce.Job: Running job: job_1579055927679_0006\n",
      "20/02/26 21:43:31 INFO mapreduce.Job: Job job_1579055927679_0006 running in uber mode : false\n",
      "20/02/26 21:43:31 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/02/26 21:43:37 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "20/02/26 21:43:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/02/26 21:43:42 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/02/26 21:43:42 INFO mapreduce.Job: Job job_1579055927679_0006 completed successfully\n",
      "20/02/26 21:43:42 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=895\n",
      "\t\tFILE: Number of bytes written=2241281\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=14720\n",
      "\t\tHDFS: Number of bytes written=25\n",
      "\t\tHDFS: Number of read operations=51\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=16\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=16\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4388774\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=295191\n",
      "\t\tTotal time spent by all map tasks (ms)=74386\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2523\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=74386\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2523\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=139250592\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9446112\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=29\n",
      "\t\tMap output records=29\n",
      "\t\tMap output bytes=1540\n",
      "\t\tMap output materialized bytes=1535\n",
      "\t\tInput split bytes=1824\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=25\n",
      "\t\tReduce shuffle bytes=1535\n",
      "\t\tReduce input records=29\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=58\n",
      "\t\tShuffled Maps =16\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=16\n",
      "\t\tGC time elapsed (ms)=2576\n",
      "\t\tCPU time spent (ms)=27920\n",
      "\t\tPhysical memory (bytes) snapshot=8505241600\n",
      "\t\tVirtual memory (bytes) snapshot=61655015424\n",
      "\t\tTotal committed heap usage (bytes)=12341739520\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=12896\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=25\n",
      "20/02/26 21:43:42 INFO streaming.StreamJob: Output directory: /user/lifeng/output\n",
      "     29     225    1540\t\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -rm -r /user/lifeng/output\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar -input /user/lifeng/asm-LICENSE  -output /user/lifeng/output -mapper \"/usr/bin/cat\" -reducer \"/usr/bin/wc\" -numReduceTasks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     29     225    1540\t\n"
     ]
    }
   ],
   "source": [
    "hadoop fs -cat /user/lifeng/output/*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
